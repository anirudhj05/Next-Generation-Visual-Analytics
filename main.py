import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import io
from streamlit_option_menu import option_menu

from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
import umap
import matplotlib.pyplot as plt
import seaborn as sns

import matplotlib.colors as mcolors
import colorsys

from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc
from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif

import joblib
import os
from sklearn.preprocessing import label_binarize
from skimage.transform import resize
from lungmask import mask
import SimpleITK as sitk
from totalsegmentator.python_api import totalsegmentator
import tempfile
import cv2

# Import the AI assistant module
from ai_assistant import display_ai_assistant

#===================================================================================
# FUNCTIONS
#===================================================================================


def generate_color_palette(base_color, num_colors):
    """
    Generate a list of visually distinct colors based on a single base color.
    Colors are generated by altering the lightness of the base hue.
    """
    if num_colors == 1:
        return [base_color]

    # Convert base color from hex to RGB
    rgb = mcolors.hex2color(base_color)
    h, l, s = colorsys.rgb_to_hls(*rgb)

    # Generate color variations by adjusting lightness
    lightness_values = [
        0.3 + 0.5 * (i / (num_colors - 1)) for i in range(num_colors)
    ]
    palette = [
        mcolors.to_hex(colorsys.hls_to_rgb(h, l_val, s))
        for l_val in lightness_values
    ]
    return palette


# Example: Generate 5 shades based on red
generate_color_palette("#FF0000", 5)


def load_image_array(path):
    image_sitk = sitk.ReadImage(path)
    image_np = sitk.GetArrayFromImage(image_sitk)  # Shape: [Z, Y, X]
    return image_np


def overlay_mask_on_image(image, mask, color="#FF0000", alpha=0.5):
    image = np.clip(image, -1000, 1000)
    image = (image - np.min(image)) / (np.max(image) - np.min(image))
    overlay = np.stack([image] * 3, axis=-1)

    hex_color = color.lstrip("#")
    rgb = tuple(int(hex_color[i:i + 2], 16) / 255 for i in (0, 2, 4))

    for i in range(3):
        overlay[...,
                i] = np.where(mask > 0,
                              alpha * rgb[i] + (1 - alpha) * overlay[..., i],
                              overlay[..., i])
    return overlay


def get_current_df():
    if "cleaned_df" in st.session_state and st.session_state.cleaned_df is not None:
        return st.session_state.cleaned_df
    else:
        return st.session_state.raw_df


import colorsys


def generate_color_variants(base_hex, n_variants):
    # Convert HEX to HLS
    base_hex = base_hex.lstrip('#')
    r, g, b = tuple(int(base_hex[i:i + 2], 16) / 255 for i in (0, 2, 4))
    h, l, s = colorsys.rgb_to_hls(r, g, b)

    variants = []
    for i in range(n_variants):
        # Adjust lightness
        new_l = max(0, min(1, l * (0.7 + 0.6 * (i / max(n_variants - 1, 1)))))
        new_r, new_g, new_b = colorsys.hls_to_rgb(h, new_l, s)
        hex_color = '#%02x%02x%02x' % (int(new_r * 255), int(
            new_g * 255), int(new_b * 255))
        variants.append(hex_color)

    return variants


#===================================================================================

# Page Configuration
st.set_page_config(page_title="Biomedical Data Visualization",
                   layout="wide",
                   initial_sidebar_state="expanded")

# Define menu options as a constant
MENU_OPTIONS = [
    "Home", "Preview", "Summary", "Clean Data", "Filter & Plot",
    "Dimensionality Reduction", "Machine Learning", "Image Analysis",
    "AI Assistant"
]
MENU_ICONS = [
    "house", "table", "bar-chart-line", "brush", "sliders",
    "arrows-angle-contract", "robot", "image", "chat-dots"
]

# Initialize session state variables if they don't exist
if "raw_df" not in st.session_state:
    st.session_state.raw_df = None
if "cleaned_df" not in st.session_state:
    st.session_state.cleaned_df = None
if "file_uploaded" not in st.session_state:
    st.session_state.file_uploaded = False

# --- Sidebar Menu ---
with st.sidebar:
    try:
        st.markdown(
            "<h1 style='text-align: center;'>Next Generation Analytics</h1>",
            unsafe_allow_html=True)
        st.image("NextGen.png", width=400)

    except:
        st.title("Next Generation Analytics")

    # Display navigation menu
    selected_page = option_menu(
        menu_title="Menu",
        options=MENU_OPTIONS,
        icons=MENU_ICONS,
        menu_icon="list",
    )

    # --- File Upload ---
    st.markdown("### üìÅ Upload your file")

    uploaded_file = st.file_uploader("Choose a file",
                                     type=["csv", "tsv", "xls", "xlsx"])

    # Save uploaded file to session state
    if uploaded_file:
        # Always try to load the file if it is newly uploaded
        if ("uploaded_filename" not in st.session_state) or (
                uploaded_file.name != st.session_state.uploaded_filename):
            try:
                name = uploaded_file.name.lower()
                if name.endswith(".csv"):
                    raw_df = pd.read_csv(uploaded_file)
                elif name.endswith(".tsv"):
                    raw_df = pd.read_csv(uploaded_file, sep="\t")
                else:
                    raw_df = pd.read_excel(uploaded_file)

                st.session_state.raw_df = raw_df.copy()
                st.session_state.cleaned_df = raw_df.copy()
                st.session_state.uploaded_filename = uploaded_file.name  # Save current uploaded filename
                st.success(
                    f"File loaded successfully: {len(raw_df)} rows, {len(raw_df.columns)} columns"
                )
            except Exception as e:
                st.error(f"Error loading file: {str(e)}")
    else:
        # Reset uploaded filename if no file is uploaded
        if "uploaded_filename" in st.session_state:
            del st.session_state.uploaded_filename

# --- Main Content ---
if selected_page == "Home":
    st.title("üìä Interactive Biomedical Data Dashboard")
    st.write(
        "This dashboard provides tools for biomedical data visualization, cleaning, AI modeling, and medical image analysis."
    )

    st.markdown("""
    ### Features:
    - **Data Preview**: View and explore your dataset
    - **Summary Statistics**: Analyze key metrics and distributions
    - **Data Cleaning**: Handle missing values and outliers
    - **Visualization**: Generate various plots for your data
    - **Dimensionality Reduction**: Apply PCA, t-SNE, and UMAP techniques
    - **Maching Learning**: Build machine learning models with built-in support for feature selection, cross-validation, and performance evaluation across multiple classifiers
    - **Image Analysis**:
        - **Medical Image Analysis**: Upload and analyze NIfTI `.nii.gz` files, navigate across slices, and apply pre-trained models like LungMask and TotalSegmentator for organ segmentation with downloadable results
        - **Natural Image Analysis**: Upload `.jpg` or `.png` files and perform basic computer vision tasks such as grayscale conversion, channel separation, blurring, edge detection, and more

    ### Getting Started:
    1. Upload your dataset or medical images using the sidebar
    2. Navigate through the different sections using the menu
    3. Analyze, visualize, segment, and model your data interactively
    4. Download results and visualizations as needed
    """)

    if st.session_state.raw_df is None:
        st.info("Please upload a dataset using the sidebar to begin.")
        st.markdown("### Supported file formats:")
        st.markdown("- CSV (.csv)")
        st.markdown("- TSV (.tsv)")
        st.markdown("- Excel (.xls, .xlsx)")
    else:
        st.success(
            f"File loaded successfully! Your dataset has {len(st.session_state.raw_df)} rows and {len(st.session_state.raw_df.columns)} columns."
        )

        # Preview of the dataset
        st.subheader("Quick Preview")
        st.dataframe(st.session_state.raw_df.head(5), use_container_width=True)

        # Show data types information
        st.subheader("Data Types Overview")
        dtypes_df = pd.DataFrame(st.session_state.raw_df.dtypes,
                                 columns=['Data Type'])
        dtypes_df.index.name = 'Column'
        st.dataframe(dtypes_df, use_container_width=True)

elif selected_page == "Preview" and st.session_state.raw_df is not None:
    df = get_current_df()

    st.title("üßæ Data Preview")

    # Options for preview
    show_rows = st.slider("Number of rows to display", 5, 100, 20)

    # Display the data
    st.subheader(f"First {show_rows} rows")
    st.dataframe(df.head(show_rows), use_container_width=True)

    # Display data types
    st.subheader("Data Information")
    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"**Rows:** {df.shape[0]}")
        st.markdown(f"**Columns:** {df.shape[1]}")

    with col2:
        num_cols = len(df.select_dtypes(include='number').columns)
        cat_cols = len(df.select_dtypes(exclude='number').columns)
        st.markdown(f"**Numeric columns:** {num_cols}")
        st.markdown(f"**Non-numeric columns:** {cat_cols}")

    # Data types
    st.subheader("Column Data Types")
    dtypes_df = pd.DataFrame(df.dtypes, columns=['Data Type'])
    dtypes_df.index.name = 'Column'
    st.dataframe(dtypes_df, use_container_width=True)

elif selected_page == "Summary" and st.session_state.raw_df is not None:
    df = get_current_df()

    # Show summary statistics
    st.subheader("Statistical Summary")
    summary_df = df.describe(include='all').T
    summary_df['count'] = summary_df['count'].astype(int)
    st.dataframe(summary_df, use_container_width=True)

    # Data distribution for numeric columns
    st.subheader("Data Distribution")
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()

    if numeric_cols:
        col_to_plot = st.selectbox("Select column to visualize", numeric_cols)

        if col_to_plot:
            col1, col2 = st.columns(2)

            with col1:
                # Histogram
                fig_hist = px.histogram(df,
                                        x=col_to_plot,
                                        title=f"Histogram of {col_to_plot}",
                                        nbins=30)
                st.plotly_chart(fig_hist, use_container_width=True)

            with col2:
                # Box plot
                fig_box = px.box(df,
                                 y=col_to_plot,
                                 title=f"Box Plot of {col_to_plot}")
                st.plotly_chart(fig_box, use_container_width=True)
    else:
        st.warning("No numeric columns found in the dataset.")

elif selected_page == "Clean Data" and st.session_state.raw_df is not None:
    df = get_current_df()

    st.title("üßº Data Cleaning & Inspection")

    # Get original and current data stats
    original_shape = st.session_state.raw_df.shape
    current_shape = df.shape

    # Display data types
    st.subheader("Data Information")
    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"**Rows:** {df.shape[0]}")
        st.markdown(f"**Columns:** {df.shape[1]}")

    with col2:
        num_cols = len(df.select_dtypes(include='number').columns)
        cat_cols = len(df.select_dtypes(exclude='number').columns)
        st.markdown(f"**Numeric columns:** {num_cols}")
        st.markdown(f"**Non-numeric columns:** {cat_cols}")

    # Missing values analysis
    st.subheader("Missing Values")
    missing_df = df.isnull().sum()
    missing_df = missing_df[missing_df > 0]

    if len(missing_df) > 0:
        st.write("üîé Missing values per column:")
        st.dataframe(missing_df, use_container_width=True)

        st.write("üîé Sample rows with missing values:")
        missing_rows = df[df.isnull().any(axis=1)]
        st.dataframe(missing_rows.head(10), use_container_width=True)
        st.info(f"Total rows with missing values: {len(missing_rows)}")
    else:
        st.success("No missing values found in the dataset!")

    # Tabs for different cleaning operations
    tab1, tab2, tab3, tab4 = st.tabs(
        ["Missing Values", "Duplicates", "Outliers", "Data Transformations"])

    with tab1:
        # Cleaning options
        st.subheader("üßπ Cleaning Options")

        # Define all cleaning operations in one place for consistency
        ALL_CLEANING_OPTIONS = [
            "Drop Rows with Missing Values",
            "Drop Columns with Missing Values", "Fill Missing Values with 0",
            "Fill Missing Values with Column Mean",
            "Fill Missing Values with Column Median",
            "Fill Missing Values with Column Mode"
        ]

        # Multi-select for cleaning operations
        cleaning_options = st.multiselect(
            "Select cleaning operations to apply (in order)",
            options=ALL_CLEANING_OPTIONS)

        col1, col2 = st.columns(2)

        reset_button = col1.button("‚Ü©Ô∏è Reset to Original Data")
        apply_button = col2.button("Apply Cleaning")

        if reset_button:
            st.session_state.cleaned_df = st.session_state.raw_df.copy()
            st.success("Data reset to original state.")
            st.rerun()

        if apply_button:
            # Reset everything - start fresh with the original data
            cleaned_df = st.session_state.raw_df.copy()

            # Starting state - captured for later comparison
            original_rows = len(cleaned_df)
            original_cols = len(cleaned_df.columns)
            original_missing = cleaned_df.isnull().sum().sum()

            # Track what operations were performed
            operations_performed = []

            # Check if any operations were selected
            if not cleaning_options:
                st.warning("Please select at least one cleaning operation.")
            else:
                # First, check if there are any missing values at all in the dataset
                total_missing = cleaned_df.isnull().sum().sum()

                if total_missing > 0:
                    # Calculate which columns have missing values before applying operations
                    cols_with_missing = cleaned_df.columns[
                        cleaned_df.isnull().any()].tolist()

                    # Create a dictionary of cleaning operations (keyed by their exact name from multiselect)
                    cleaning_functions = {
                        "Drop Rows with Missing Values": {
                            "function": lambda df: drop_rows_with_missing(df),
                            "description":
                            "Dropped {} rows with missing values"
                        },
                        "Drop Columns with Missing Values": {
                            "function":
                            lambda df: drop_columns_with_missing(df),
                            "description":
                            "Dropped {} columns with missing values: {}"
                        },
                        "Fill Missing Values with 0": {
                            "function": lambda df: fill_with_zero(df),
                            "description": "Filled {} missing values with 0"
                        },
                        "Fill Missing Values with Column Mean": {
                            "function":
                            lambda df: fill_with_mean(df),
                            "description":
                            "Filled {} missing values with column means"
                        },
                        "Fill Missing Values with Column Median": {
                            "function":
                            lambda df: fill_with_median(df),
                            "description":
                            "Filled {} missing values with column medians"
                        },
                        "Fill Missing Values with Column Mode": {
                            "function":
                            lambda df: fill_with_mode(df),
                            "description":
                            "Filled {} missing values with column modes"
                        }
                    }

                    # Functions for each cleaning operation
                    def drop_rows_with_missing(df):
                        row_count_before = len(df)
                        df_result = df.dropna()
                        rows_dropped = row_count_before - len(df_result)
                        result_message = f"Dropped {rows_dropped} rows with missing values"
                        return df_result, result_message

                    def drop_columns_with_missing(df):
                        # Get columns with missing values in the CURRENT state of df
                        current_cols_with_missing = df.columns[
                            df.isnull().any()].tolist()
                        if current_cols_with_missing:
                            cols_before = set(df.columns)
                            df_result = df.drop(
                                columns=current_cols_with_missing)
                            cols_dropped = cols_before - set(df_result.columns)
                            result_message = f"Dropped {len(cols_dropped)} columns with missing values: {', '.join(cols_dropped)}"
                            return df_result, result_message
                        else:
                            return df, "No columns with missing values to drop"

                    def fill_with_zero(df):
                        missing_before = df.isnull().sum().sum()

                        if missing_before > 0:
                            # Fill missing values with 0
                            df_result = df.fillna(0)

                            # Verify fill
                            missing_after = df_result.isnull().sum().sum()
                            filled_count = missing_before - missing_after

                            result_message = f"Filled {filled_count} missing values with 0"
                            return df_result, result_message
                        else:
                            return df, "No missing values to fill with 0"

                    def fill_with_mean(df):
                        missing_before = df.isnull().sum().sum()

                        # Get only numeric columns
                        numeric_df = df.select_dtypes(include=['number'])

                        if not numeric_df.empty and missing_before > 0:
                            # Make a copy to avoid modifying original during iteration
                            df_result = df.copy()
                            filled_count = 0

                            # For each numeric column with missing values
                            for col in numeric_df.columns:
                                if df[col].isnull().sum() > 0:
                                    col_mean = df[col].mean()
                                    df_result[col] = df_result[col].fillna(
                                        col_mean)
                                    filled_this_column = df[col].isnull().sum()
                                    filled_count += filled_this_column

                            if filled_count > 0:
                                result_message = f"Filled {filled_count} missing values with column means"
                                return df_result, result_message
                            else:
                                return df, "No numeric columns with missing values to fill with mean"
                        else:
                            return df, "No numeric columns with missing values to fill with mean"

                    def fill_with_median(df):
                        missing_before = df.isnull().sum().sum()

                        # Get only numeric columns
                        numeric_df = df.select_dtypes(include=['number'])

                        if not numeric_df.empty and missing_before > 0:
                            # Make a copy to avoid modifying original during iteration
                            df_result = df.copy()
                            filled_count = 0

                            # For each numeric column with missing values
                            for col in numeric_df.columns:
                                if df[col].isnull().sum() > 0:
                                    col_median = df[col].median()
                                    df_result[col] = df_result[col].fillna(
                                        col_median)
                                    filled_this_column = df[col].isnull().sum()
                                    filled_count += filled_this_column

                            if filled_count > 0:
                                result_message = f"Filled {filled_count} missing values with column medians"
                                return df_result, result_message
                            else:
                                return df, "No numeric columns with missing values to fill with median"
                        else:
                            return df, "No numeric columns with missing values to fill with median"

                    def fill_with_mode(df):
                        missing_before = df.isnull().sum().sum()

                        if missing_before > 0:
                            # Make a copy to avoid modifying original during iteration
                            df_result = df.copy()
                            filled_count = 0

                            # For each column with missing values
                            for col in df.columns:
                                if df[col].isnull().sum() > 0:
                                    # Get mode - mode() returns a Series, so use [0] to get the first mode
                                    try:
                                        col_mode = df[col].mode()[0]
                                        df_result[col] = df_result[col].fillna(
                                            col_mode)
                                        filled_this_column = df[col].isnull(
                                        ).sum()
                                        filled_count += filled_this_column
                                    except:
                                        # Skip columns where mode can't be calculated
                                        continue

                            if filled_count > 0:
                                result_message = f"Filled {filled_count} missing values with column modes"
                                return df_result, result_message
                            else:
                                return df, "No columns with missing values to fill with mode"
                        else:
                            return df, "No missing values to fill with mode"

                    # Apply selected operations in order
                    for op_name in cleaning_options:
                        if op_name in cleaning_functions:
                            # Apply the function
                            cleaned_df, message = cleaning_functions[op_name][
                                "function"](cleaned_df)
                            operations_performed.append(message)

                    # Update session state with cleaned dataframe
                    st.session_state.cleaned_df = cleaned_df

                    # Show results
                    st.success("Cleaning operations completed!")
                    st.subheader("Operations performed:")
                    for op in operations_performed:
                        st.markdown(f"- {op}")

                    # Final stats
                    final_rows = len(cleaned_df)
                    final_cols = len(cleaned_df.columns)
                    final_missing = cleaned_df.isnull().sum().sum()

                    st.subheader("Summary of changes:")
                    st.markdown(
                        f"- Rows: {original_rows} ‚Üí {final_rows} ({original_rows - final_rows} removed)"
                    )
                    st.markdown(
                        f"- Columns: {original_cols} ‚Üí {final_cols} ({original_cols - final_cols} removed)"
                    )
                    st.markdown(
                        f"- Missing values: {original_missing} ‚Üí {final_missing} ({original_missing - final_missing} filled or removed)"
                    )

                else:
                    st.success("Your dataset has no missing values to clean!")

            # Preview cleaned data
            st.subheader("Preview of Cleaned Data")
            st.dataframe(cleaned_df.head(10), use_container_width=True)

    with tab2:
        st.subheader("Handle Duplicate Rows")

        # Check for duplicate rows
        dup_count = df.duplicated().sum()

        if dup_count == 0:
            st.success("No duplicate rows found in the dataset!")
        else:
            st.warning(f"Found {dup_count} duplicate rows in the dataset.")

            # Show duplicate rows
            st.write("Duplicate rows:")
            st.dataframe(df[df.duplicated(keep='first')])

            if st.button("Remove Duplicate Rows"):
                updated_df = st.session_state.cleaned_df.drop_duplicates(
                    keep='first')
                st.session_state.cleaned_df = updated_df
                st.success(
                    f"Removed {dup_count} duplicate rows from the dataset.")

    with tab3:
        st.subheader("Handle Outliers")

        # Only show numeric columns for outlier detection
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()

        if not numeric_cols:
            st.warning("No numeric columns found for outlier detection.")
        else:
            selected_col = st.selectbox("Select column for outlier detection",
                                        numeric_cols,
                                        key="outlier_col")

            # Calculate IQR for the selected column
            Q1 = df[selected_col].quantile(0.25)
            Q3 = df[selected_col].quantile(0.75)
            IQR = Q3 - Q1

            # Define outlier bounds
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            # Identify outliers
            outliers = df[(df[selected_col] < lower_bound) |
                          (df[selected_col] > upper_bound)]
            outlier_count = len(outliers)

            # Display statistics
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total Rows", len(df))
            with col2:
                st.metric("Outliers", outlier_count)
            with col3:
                st.metric("Outlier Percentage",
                          f"{(outlier_count / len(df) * 100):.2f}%")

            # Display boxplot to visualize outliers
            fig = px.box(df,
                         y=selected_col,
                         title=f"Box Plot of {selected_col} showing Outliers")
            st.plotly_chart(fig, use_container_width=True)

            if outlier_count > 0:
                st.write("Outlier rows:")
                st.dataframe(outliers)

                # Options for handling outliers
                outlier_strategy = st.radio(
                    "Select strategy for handling outliers", [
                        "Remove outliers", "Cap outliers (winsorize)",
                        "Keep outliers"
                    ])

                if outlier_strategy == "Remove outliers" and st.button(
                        "Apply - Remove Outliers"):
                    st.session_state.cleaned_df = st.session_state.cleaned_df[
                        (st.session_state.cleaned_df[selected_col] >=
                         lower_bound)
                        & (st.session_state.cleaned_df[selected_col] <=
                           upper_bound)]
                    st.success(
                        f"Removed {outlier_count} outlier rows from the dataset."
                    )

                elif outlier_strategy == "Cap outliers (winsorize)" and st.button(
                        "Apply - Cap Outliers"):
                    # Apply capping
                    st.session_state.cleaned_df[
                        selected_col] = st.session_state.cleaned_df[
                            selected_col].clip(lower=lower_bound,
                                               upper=upper_bound)
                    st.success(
                        f"Capped outliers in column '{selected_col}' to range [{lower_bound:.2f}, {upper_bound:.2f}]"
                    )

    with tab4:
        st.subheader("Data Transformations")

        # Get numeric columns
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()

        if not numeric_cols:
            st.warning("No numeric columns found for transformations.")
        else:
            transform_options = st.selectbox("Select transformation type", [
                "Normalize (Min-Max Scaling)", "Standardize (Z-score)",
                "Log Transform", "Square Root Transform"
            ])

            selected_col = st.selectbox("Select column to transform",
                                        numeric_cols,
                                        key="transform_col")

            col1, col2 = st.columns(2)

            with col1:
                st.write("Original data distribution:")
                fig_orig = px.histogram(df,
                                        x=selected_col,
                                        title=f"Original {selected_col}")
                st.plotly_chart(fig_orig, use_container_width=True)

            with col2:
                if transform_options == "Normalize (Min-Max Scaling)":
                    # Apply min-max scaling
                    scaler = MinMaxScaler()
                    transformed_data = scaler.fit_transform(df[[selected_col]])
                    transformed_df = pd.DataFrame(
                        transformed_data,
                        columns=[f"{selected_col}_normalized"])

                    st.write("Normalized data distribution:")
                    fig_norm = px.histogram(transformed_df,
                                            x=f"{selected_col}_normalized",
                                            title=f"Normalized {selected_col}")
                    st.plotly_chart(fig_norm, use_container_width=True)

                    if st.button("Apply - Normalize Data"):
                        st.session_state.cleaned_df[
                            f"{selected_col}_normalized"] = scaler.fit_transform(
                                st.session_state.cleaned_df[[selected_col]])
                        st.success(
                            f"Added normalized column '{selected_col}_normalized' to the dataset."
                        )

                elif transform_options == "Standardize (Z-score)":
                    # Apply standardization
                    scaler = StandardScaler()
                    transformed_data = scaler.fit_transform(df[[selected_col]])
                    transformed_df = pd.DataFrame(
                        transformed_data,
                        columns=[f"{selected_col}_standardized"])

                    st.write("Standardized data distribution:")
                    fig_std = px.histogram(
                        transformed_df,
                        x=f"{selected_col}_standardized",
                        title=f"Standardized {selected_col}")
                    st.plotly_chart(fig_std, use_container_width=True)

                    if st.button("Apply - Standardize Data"):
                        st.session_state.cleaned_df[
                            f"{selected_col}_standardized"] = scaler.fit_transform(
                                st.session_state.cleaned_df[[selected_col]])
                        st.success(
                            f"Added standardized column '{selected_col}_standardized' to the dataset."
                        )

                elif transform_options == "Log Transform":
                    # Check if data is suitable for log transform
                    if df[selected_col].min() <= 0:
                        st.error(
                            "Log transform requires positive values. This column contains zero or negative values."
                        )
                    else:
                        # Apply log transform
                        transformed_df = pd.DataFrame(
                            np.log(df[selected_col]),
                            columns=[f"{selected_col}_log"])

                        st.write("Log-transformed data distribution:")
                        fig_log = px.histogram(
                            transformed_df,
                            x=f"{selected_col}_log",
                            title=f"Log Transformed {selected_col}")
                        st.plotly_chart(fig_log, use_container_width=True)

                        if st.button("Apply - Log Transform"):
                            st.session_state.cleaned_df[
                                f"{selected_col}_log"] = np.log(
                                    st.session_state.cleaned_df[selected_col])
                            st.success(
                                f"Added log-transformed column '{selected_col}_log' to the dataset."
                            )

                elif transform_options == "Square Root Transform":
                    # Check if data is suitable for sqrt transform
                    if df[selected_col].min() < 0:
                        st.error(
                            "Square root transform requires non-negative values. This column contains negative values."
                        )
                    else:
                        # Apply sqrt transform
                        transformed_df = pd.DataFrame(
                            np.sqrt(df[selected_col]),
                            columns=[f"{selected_col}_sqrt"])

                        st.write("Square root transformed data distribution:")
                        fig_sqrt = px.histogram(
                            transformed_df,
                            x=f"{selected_col}_sqrt",
                            title=f"Square Root Transformed {selected_col}")
                        st.plotly_chart(fig_sqrt, use_container_width=True)

                        if st.button("Apply - Square Root Transform"):
                            st.session_state.cleaned_df[
                                f"{selected_col}_sqrt"] = np.sqrt(
                                    st.session_state.cleaned_df[selected_col])
                            st.success(
                                f"Added square root transformed column '{selected_col}_sqrt' to the dataset."
                            )

    # Add a small separation line
    st.markdown("---")

    # Sticky download button section
    st.markdown(
        """
        <style>
        .sticky-download {
            position: fixed;
            bottom: 30px;
            right: 30px;
            z-index: 9999;
        }
        .sticky-download button {
            background-color: #4CAF50;
            color: white;
            padding: 10px 20px;
            border-radius: 10px;
            border: none;
            font-size: 16px;
            font-weight: bold;
            box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.25);
            cursor: pointer;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    # Create a container for the sticky button
    with st.container():
        if st.session_state.cleaned_df is not None:
            # Add a small radio button for format selection
            with st.expander("‚¨áÔ∏è Final Download Options", expanded=False):
                download_format = st.radio("Select format", ["CSV", "Excel"],
                                           horizontal=True)

            # Place download button inside a div with the sticky class
            st.markdown('<div class="sticky-download">',
                        unsafe_allow_html=True)

            if download_format == "CSV":
                csv_data = st.session_state.cleaned_df.to_csv(
                    index=False).encode('utf-8')
                st.download_button(
                    label="üì• Download CSV",
                    data=csv_data,
                    file_name="cleaned_data.csv",
                    mime="text/csv",
                    use_container_width=False,
                )
            else:
                excel_data = BytesIO()
                st.session_state.cleaned_df.to_excel(excel_data,
                                                     index=False,
                                                     engine='xlsxwriter')
                excel_data.seek(0)
                st.download_button(
                    label="üì• Download Excel",
                    data=excel_data,
                    file_name="cleaned_data.xlsx",
                    mime=
                    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=False,
                )

            st.markdown('</div>', unsafe_allow_html=True)
        else:
            st.info("‚ö° No cleaned data available yet.")

elif selected_page == "Filter & Plot" and st.session_state.raw_df is not None:
    df = get_current_df()

    st.title("üìä Filter & Plot Data")

    # --- Filter Data Section ---
    st.subheader("Filter Data")

    # Get column types
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
    categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()
    filtered_df = df.copy()

    # --- Data Visualization Section ---
    st.subheader("Data Visualization")

    # Create tabs for different plot types
    plot_tab1, plot_tab2, plot_tab3, plot_tab4 = st.tabs([
        "Scatter & Line Plots", "Bar & Histogram Charts", "Box & Violin Plots",
        "Pie & Heatmap Charts"
    ])

    with plot_tab1:
        st.markdown("#### Scatter & Line Plots")

        plot_type = st.radio("Select plot type",
                             ["Scatter Plot", "Line Plot", "Bubble Plot"],
                             horizontal=True)

        if plot_type in ["Scatter Plot", "Line Plot", "Bubble Plot"]:
            base_color = st.color_picker("Pick a base color",
                                         value="#1f77b4",
                                         key="scatter_line_base_color")

            # Select Color by only once here
            color_col_option = st.selectbox("Color by (optional)",
                                            ["None"] + categorical_cols)
            color_col = None if color_col_option == "None" else color_col_option

            # Generate color sequence
            if color_col:
                n_classes = filtered_df[color_col].nunique()
                color_sequence = generate_color_variants(base_color, n_classes)
            else:
                color_sequence = [base_color]

            # Now plotting
            if len(numeric_cols) >= 2:
                col1, col2 = st.columns(2)

                with col1:
                    x_col = st.selectbox("X-axis column",
                                         numeric_cols,
                                         key="scatter_x")
                with col2:
                    y_col = st.selectbox("Y-axis column",
                                         numeric_cols,
                                         key="scatter_y")

                # Size option (only for Bubble Plot)
                size_col = None
                if plot_type == "Bubble Plot" and len(numeric_cols) > 2:
                    size_col = st.selectbox("Size by (optional)",
                                            ["None"] + numeric_cols,
                                            key="bubble_size")
                    size_col = None if size_col == "None" else size_col

                # Now plot based on type
                if plot_type == "Scatter Plot":
                    fig = px.scatter(filtered_df,
                                     x=x_col,
                                     y=y_col,
                                     color=color_col,
                                     color_discrete_sequence=color_sequence,
                                     title=f"{x_col} vs {y_col}",
                                     labels={
                                         x_col: x_col,
                                         y_col: y_col
                                     })
                    st.plotly_chart(fig, use_container_width=True)

                elif plot_type == "Line Plot":
                    fig = px.line(filtered_df.sort_values(x_col),
                                  x=x_col,
                                  y=y_col,
                                  color=color_col,
                                  color_discrete_sequence=color_sequence,
                                  title=f"{x_col} vs {y_col}",
                                  labels={
                                      x_col: x_col,
                                      y_col: y_col
                                  })
                    st.plotly_chart(fig, use_container_width=True)

                elif plot_type == "Bubble Plot":
                    fig = px.scatter(filtered_df,
                                     x=x_col,
                                     y=y_col,
                                     size=size_col,
                                     color=color_col,
                                     color_discrete_sequence=color_sequence,
                                     title=f"{x_col} vs {y_col}",
                                     labels={
                                         x_col: x_col,
                                         y_col: y_col
                                     })
                    st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning(
                    "You need at least two numeric columns for this plot type."
                )

    with plot_tab2:
        st.markdown("#### Bar & Histogram Charts")

        plot_type = st.radio("Select plot type",
                             ["Bar Chart", "Histogram", "Grouped Bar Chart"],
                             horizontal=True)

        base_color = st.color_picker(
            "Pick a base color", value="#1f77b4",
            key="bar_hist_base_color")  # <-- Always one base color

        if plot_type == "Bar Chart":
            if categorical_cols:
                x_col = st.selectbox("X-axis column (categorical)",
                                     categorical_cols,
                                     key="bar_x")
                y_agg = st.selectbox("Y-axis aggregation",
                                     ["Count", "Sum", "Mean", "Median"],
                                     key="bar_agg")

                # Optionally select a numeric column to aggregate
                y_col = None
                if y_agg != "Count" and numeric_cols:
                    y_col = st.selectbox("Numeric column to aggregate",
                                         numeric_cols,
                                         key="bar_y")

                # Prepare data for the bar chart
                if y_agg == "Count":
                    plot_df = filtered_df[x_col].value_counts().reset_index()
                    plot_df.columns = [x_col, 'Count']
                    fig = px.bar(
                        plot_df,
                        x=x_col,
                        y='Count',
                        color_discrete_sequence=[base_color],  # Single color
                        title=f"Count of {x_col}",
                        labels={
                            x_col: x_col,
                            'Count': 'Count'
                        })
                else:
                    agg_funcs = {
                        'Sum': 'sum',
                        'Mean': 'mean',
                        'Median': 'median'
                    }
                    plot_df = filtered_df.groupby(x_col)[y_col].agg(
                        agg_funcs[y_agg]).reset_index()
                    fig = px.bar(
                        plot_df,
                        x=x_col,
                        y=y_col,
                        color_discrete_sequence=[base_color],  # Single color
                        title=f"{y_agg} of {y_col} by {x_col}",
                        labels={
                            x_col: x_col,
                            y_col: f"{y_agg} of {y_col}"
                        })

                st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning(
                    "You need at least one categorical column for a bar chart."
                )

        elif plot_type == "Histogram":
            if numeric_cols:
                col_to_plot = st.selectbox("Select column",
                                           numeric_cols,
                                           key="hist_col")
                num_bins = st.slider("Number of bins",
                                     5,
                                     100,
                                     20,
                                     key="hist_bins")

                fig = px.histogram(
                    filtered_df,
                    x=col_to_plot,
                    nbins=num_bins,
                    color_discrete_sequence=[base_color],  # Single color
                    title=f"Histogram of {col_to_plot}",
                    labels={
                        col_to_plot: col_to_plot,
                        'count': 'Count'
                    })
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning(
                    "You need at least one numeric column for a histogram.")

        elif plot_type == "Grouped Bar Chart":
            if categorical_cols and len(categorical_cols) >= 2:
                x_col = st.selectbox("X-axis column",
                                     categorical_cols,
                                     key="grouped_x")
                group_col = st.selectbox(
                    "Group by column",
                    [c for c in categorical_cols if c != x_col],
                    key="grouped_group")

                agg_option = st.selectbox("Y-axis aggregation",
                                          ["Count", "Sum", "Mean", "Median"],
                                          key="grouped_agg")
                y_col = None
                if agg_option != "Count" and numeric_cols:
                    y_col = st.selectbox("Numeric column to aggregate",
                                         numeric_cols,
                                         key="grouped_y")

                # Prepare data for the grouped bar chart
                if agg_option == "Count":
                    plot_df = filtered_df.groupby(
                        [x_col, group_col]).size().reset_index(name='Count')
                    n_classes = plot_df[group_col].nunique()
                    color_sequence = generate_color_variants(
                        base_color, n_classes)

                    fig = px.bar(
                        plot_df,
                        x=x_col,
                        y='Count',
                        color=group_col,
                        barmode='group',
                        color_discrete_sequence=color_sequence,
                        title=f"Count of {x_col} grouped by {group_col}",
                        labels={
                            x_col: x_col,
                            'Count': 'Count',
                            group_col: group_col
                        })
                else:
                    agg_funcs = {
                        'Sum': 'sum',
                        'Mean': 'mean',
                        'Median': 'median'
                    }
                    plot_df = filtered_df.groupby([
                        x_col, group_col
                    ])[y_col].agg(agg_funcs[agg_option]).reset_index()
                    n_classes = plot_df[group_col].nunique()
                    color_sequence = generate_color_variants(
                        base_color, n_classes)

                    fig = px.bar(
                        plot_df,
                        x=x_col,
                        y=y_col,
                        color=group_col,
                        barmode='group',
                        color_discrete_sequence=color_sequence,
                        title=
                        f"{agg_option} of {y_col} by {x_col} grouped by {group_col}",
                        labels={
                            x_col: x_col,
                            y_col: f"{agg_option} of {y_col}",
                            group_col: group_col
                        })

                st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning(
                    "You need at least two categorical columns for a grouped bar chart."
                )

    with plot_tab3:
        st.markdown("#### Box & Violin Plots")

        plot_type = st.radio("Select plot type", ["Box Plot", "Violin Plot"],
                             horizontal=True)

        base_color = st.color_picker(
            "Pick a base color", value="#1f77b4",
            key="box_violin_base_color")  # Always pick base color

        if numeric_cols:
            y_col = st.selectbox("Y-axis column (numeric)",
                                 numeric_cols,
                                 key="box_y")

            # Optional grouping by categorical column
            use_category = False
            x_col = None
            if categorical_cols:
                use_category = st.checkbox("Group by category",
                                           value=True,
                                           key="box_use_category")
                if use_category:
                    x_col = st.selectbox("X-axis column (categorical)",
                                         categorical_cols,
                                         key="box_x")

            # Determine color sequence
            if use_category and x_col:
                n_classes = filtered_df[x_col].nunique()
                color_sequence = generate_color_variants(base_color, n_classes)
            else:
                color_sequence = [base_color]

            if plot_type == "Box Plot":
                fig = px.box(
                    filtered_df,
                    x=x_col if use_category else None,
                    y=y_col,
                    color=x_col if use_category else None,
                    color_discrete_sequence=color_sequence,
                    title=f"Box Plot of {y_col}" +
                    (f" by {x_col}" if use_category and x_col else ""),
                    labels={
                        x_col: x_col,
                        y_col: y_col
                    } if use_category and x_col else {y_col: y_col})
                st.plotly_chart(fig, use_container_width=True)

            elif plot_type == "Violin Plot":
                fig = px.violin(
                    filtered_df,
                    x=x_col if use_category else None,
                    y=y_col,
                    color=x_col if use_category else None,
                    color_discrete_sequence=color_sequence,
                    box=True,  # show box inside violin
                    title=f"Violin Plot of {y_col}" +
                    (f" by {x_col}" if use_category and x_col else ""),
                    labels={
                        x_col: x_col,
                        y_col: y_col
                    } if use_category and x_col else {y_col: y_col})
                st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning(
                "You need at least one numeric column for these plot types.")

    with plot_tab4:
        st.markdown("#### Pie & Heatmap Charts")

        plot_type = st.radio("Select plot type", ["Pie Chart", "Heatmap"],
                             horizontal=True)

        base_color = st.color_picker(
            "Pick a base color", value="#1f77b4",
            key="pie_heatmap_base_color")  # Always pick base color

        if plot_type == "Pie Chart":
            if categorical_cols:
                cat_col = st.selectbox("Category column",
                                       categorical_cols,
                                       key="pie_cat")

                # Optional value column
                value_type = "Count"
                value_col = None
                if numeric_cols:
                    value_type = st.selectbox(
                        "Value type", ["Count", "Sum of numeric column"],
                        key="pie_val_type")
                    if value_type == "Sum of numeric column":
                        value_col = st.selectbox("Numeric column",
                                                 numeric_cols,
                                                 key="pie_val")

                # Prepare data for the pie chart
                if value_type == "Count":
                    plot_df = filtered_df[cat_col].value_counts().reset_index()
                    plot_df.columns = [cat_col, 'Count']
                    values_col = 'Count'
                else:  # Sum of numeric column
                    plot_df = filtered_df.groupby(
                        cat_col)[value_col].sum().reset_index()
                    values_col = value_col

                # Generate color variants for the categories
                n_classes = plot_df[cat_col].nunique()
                color_sequence = generate_color_variants(base_color, n_classes)

                fig = px.pie(
                    plot_df,
                    names=cat_col,
                    values=values_col,
                    title=
                    f"{'Distribution' if value_type == 'Count' else f'Sum of {value_col}'} by {cat_col}",
                    color_discrete_sequence=color_sequence)

                fig.update_traces(textposition='inside',
                                  textinfo='percent+label')
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning(
                    "You need at least one categorical column for a pie chart."
                )

        elif plot_type == "Heatmap":
            if len(numeric_cols) >= 2:
                corr_type = st.selectbox("Correlation type",
                                         ["Pearson", "Spearman", "Kendall"],
                                         key="heatmap_corr")

                # Calculate correlation matrix
                corr_method = corr_type.lower()
                corr_matrix = filtered_df[numeric_cols].corr(
                    method=corr_method)

                # Instead of RdBu_r fixed scale, create a simple continuous color from selected base color
                # We'll just stretch it darker to lighter based on your base_color
                heatmap_colorscale = [[0, base_color],
                                      [1, "white"]]  # from your color to white

                fig = px.imshow(corr_matrix,
                                text_auto=True,
                                color_continuous_scale=heatmap_colorscale,
                                title=f"{corr_type} Correlation Heatmap",
                                labels={'color': 'Correlation'})

                st.plotly_chart(fig, use_container_width=True)

                st.markdown("**Correlation Matrix (Table View)**")
                st.dataframe(corr_matrix.round(2), use_container_width=True)
            else:
                st.warning(
                    "You need at least two numeric columns for a correlation heatmap."
                )

elif selected_page == "Dimensionality Reduction" and st.session_state.raw_df is not None:
    df = get_current_df()

    st.title("üìâ Dimensionality Reduction")

    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()

    if len(numeric_cols) < 3:
        st.warning(
            "Dimensionality reduction typically requires at least 3 numeric features."
        )
    else:
        st.subheader("1. Select Features")
        selected_features = st.checkbox("Select all numeric columns",
                                        value=True)

        if selected_features:
            selected_features = numeric_cols
        else:
            selected_features = st.multiselect(
                "Select numeric features to use",
                numeric_cols,
                default=numeric_cols[:min(5, len(numeric_cols))])

        if len(selected_features) < 2:
            st.warning("Please select at least 2 features to continue.")
        else:
            X = df[selected_features].copy()

            if X.isnull().any().any():
                st.warning("Missing values detected. Filling with mean.")
                X = X.fillna(X.mean())

            standardize = st.checkbox("Standardize features (recommended)",
                                      value=True)
            if standardize:
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
            else:
                X_scaled = X.values

            st.subheader("2. Choose Dimensionality Reduction Method")
            method = st.selectbox("Select method", ["PCA", "t-SNE", "UMAP"])

            st.subheader("3. Configure Parameters")

            # Pick base color manually
            base_color = st.color_picker("Pick a base color",
                                         value="#1f77b4",
                                         key="dim_red_base_color")

            if method == "PCA":
                n_components = st.slider("Number of components", 2,
                                         min(10, len(selected_features)), 2)

                pca = PCA(n_components=n_components)
                X_reduced = pca.fit_transform(X_scaled)

                explained_variance = pca.explained_variance_ratio_ * 100
                cumulative_variance = np.cumsum(explained_variance)

                variance_df = pd.DataFrame({
                    'Component': [f"PC{i+1}" for i in range(n_components)],
                    'Explained Variance (%)':
                    explained_variance,
                    'Cumulative Variance (%)':
                    cumulative_variance
                })

                fig = go.Figure()
                fig.add_trace(
                    go.Bar(x=variance_df['Component'],
                           y=variance_df['Explained Variance (%)'],
                           name='Individual Variance'))
                fig.add_trace(
                    go.Scatter(x=variance_df['Component'],
                               y=variance_df['Cumulative Variance (%)'],
                               mode='lines+markers',
                               name='Cumulative Variance'))
                fig.update_layout(title="Explained Variance by Components",
                                  xaxis_title="Component",
                                  yaxis_title="Variance (%)")
                st.plotly_chart(fig, use_container_width=True)

                st.subheader("Feature Loadings")
                loadings_df = pd.DataFrame(
                    pca.components_.T,
                    columns=[f"PC{i+1}" for i in range(n_components)],
                    index=selected_features)
                st.dataframe(loadings_df.round(3), use_container_width=True)

                fig_heatmap = px.imshow(
                    loadings_df,
                    labels=dict(x="Component", y="Feature", color="Loading"),
                    color_continuous_scale=[[0, base_color], [1, "white"]],
                    text_auto=".2f")
                st.plotly_chart(fig_heatmap, use_container_width=True)

            elif method == "t-SNE":
                perplexity = st.slider("Perplexity", 5, 50, 30)
                n_iter = st.slider("Iterations", 250, 2000, 1000, step=50)
                tsne = TSNE(n_components=2,
                            perplexity=perplexity,
                            n_iter=n_iter,
                            random_state=42)
                X_reduced = tsne.fit_transform(X_scaled)

            elif method == "UMAP":
                n_neighbors = st.slider("Neighbors", 2, 100, 15)
                min_dist = st.slider("Minimum distance",
                                     0.01,
                                     0.99,
                                     0.1,
                                     step=0.01)
                reducer = umap.UMAP(n_neighbors=n_neighbors,
                                    min_dist=min_dist,
                                    random_state=42)
                X_reduced = reducer.fit_transform(X_scaled)

            # Visualization
            st.subheader("4. Visualization")

            if X_reduced.shape[1] >= 2:
                dim_df = pd.DataFrame({
                    'Component 1': X_reduced[:, 0],
                    'Component 2': X_reduced[:, 1]
                })

                color_col = None
                if df.select_dtypes(exclude=['number']).columns.tolist():
                    use_color = st.checkbox("Color points by category",
                                            value=True)
                    if use_color:
                        col1, col2 = st.columns(2)
                        with col1:
                            color_col = st.selectbox(
                                "Select category",
                                df.select_dtypes(
                                    exclude=['number']).columns.tolist())
                            dim_df['Color'] = df[color_col].values
                        
                        # Color palette selection for better accessibility and colorblind-friendly options
                        with col2:
                            color_palette = st.selectbox(
                                "Select color palette (colorblind-friendly options available)",
                                [
                                    "Default", 
                                    "Colorblind Safe", 
                                    "Pastel", 
                                    "Bold", 
                                    "High Contrast"
                                ],
                                help="Choose a color palette that works well for your visualization needs. These options are designed to be colorblind-friendly with distinct colors."
                            )
                    else:
                        color_palette = "Default"
                else:
                    color_palette = "Default"
                    st.info("Select a label column to enable color palette selection")

                if color_col:
                    n_classes = dim_df['Color'].nunique()
                    
                    # Define colorblind-friendly palettes
                    if color_palette == "Default":
                        color_sequence = generate_color_variants(base_color, n_classes)
                    elif color_palette == "Colorblind Safe":
                        color_sequence = px.colors.qualitative.Plotly[:n_classes] if n_classes <= 10 else px.colors.qualitative.Alphabet[:n_classes]
                    elif color_palette == "Pastel":
                        color_sequence = px.colors.qualitative.Pastel[:n_classes] if n_classes <= 10 else px.colors.qualitative.Pastel1[:n_classes]
                    elif color_palette == "Bold":
                        color_sequence = px.colors.qualitative.Bold[:n_classes] if n_classes <= 10 else px.colors.qualitative.Dark24[:n_classes]
                    elif color_palette == "High Contrast":
                        color_sequence = px.colors.qualitative.T10[:n_classes] if n_classes <= 10 else px.colors.qualitative.Light24[:n_classes]
                    
                    fig = px.scatter(
                        dim_df,
                        x='Component 1',
                        y='Component 2',
                        color='Color',
                        color_discrete_sequence=color_sequence,
                        title=f"{method} Visualization Colored by {color_col} ({color_palette} palette)")
                else:
                    fig = px.scatter(dim_df,
                                     x='Component 1',
                                     y='Component 2',
                                     color_discrete_sequence=[base_color],
                                     title=f"{method} Visualization")

                st.plotly_chart(fig, use_container_width=True)

                # 3D Visualization
                if X_reduced.shape[1] >= 3:
                    show_3d = st.checkbox("Show 3D visualization", value=False)
                    if show_3d:
                        dim_df['Component 3'] = X_reduced[:, 2]
                        if color_col:
                            fig_3d = px.scatter_3d(
                                dim_df,
                                x='Component 1',
                                y='Component 2',
                                z='Component 3',
                                color='Color',
                                color_discrete_sequence=color_sequence,
                                title=
                                f"3D {method} Visualization Colored by {color_col} ({color_palette} palette)"
                            )
                        else:
                            fig_3d = px.scatter_3d(
                                dim_df,
                                x='Component 1',
                                y='Component 2',
                                z='Component 3',
                                color_discrete_sequence=[base_color],
                                title=f"3D {method} Visualization")
                        st.plotly_chart(fig_3d, use_container_width=True)

            # Add reduced dimensions to dataset
            st.subheader("5. Add Reduced Dimensions")
            if st.checkbox("Add reduced dimensions to dataset"):
                n_dims_to_add = st.slider("Dimensions to add", 1,
                                          X_reduced.shape[1], 2)
                prefix = {"PCA": "PC", "t-SNE": "TSNE", "UMAP": "UMAP"}
                for i in range(n_dims_to_add):
                    col_name = f"{prefix[method]}{i+1}"
                    st.session_state.cleaned_df[col_name] = X_reduced[:, i]
                st.success(
                    f"Added {n_dims_to_add} {method} components to the dataset!"
                )
                st.dataframe(st.session_state.cleaned_df.head(),
                             use_container_width=True)

elif selected_page == "Machine Learning" and st.session_state.raw_df is not None:
    df = get_current_df()

    st.title("ü§ñ Machine Learning Models")

    # Get column types
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
    all_cols = df.columns.tolist()

    if len(numeric_cols) < 2:
        st.error(
            "Not enough numeric features. Machine learning requires at least 2 numeric features."
        )
    else:
        # 1. Select target variable
        st.subheader("1. Select Target Variable")
        target_col = st.selectbox("Choose target column (any column)",
                                  all_cols)

        # Check if target variable has enough classes
        target_values = df[target_col].dropna().unique()
        if len(target_values) < 2:
            st.error(
                f"Target variable '{target_col}' must have at least 2 classes/values for classification."
            )
        else:
            # Display target distribution
            st.write(f"Target Variable: **{target_col}**")
            target_counts = df[target_col].value_counts().reset_index()
            target_counts.columns = [target_col, 'Count']

            fig = px.bar(
                target_counts,
                x=target_col,
                y='Count',
                title=f"Distribution of Target Variable: {target_col}")
            st.plotly_chart(fig, use_container_width=True)

            # 2. Select features
            st.subheader("2. Select Features")

            # Exclude target from potential features
            potential_features = [
                col for col in numeric_cols if col != target_col
            ]

            # Feature selection method
            feature_selection_method = st.radio("Feature selection method", [
                "Manual selection", "Automatic selection (ANOVA F-value)",
                "Automatic selection (Mutual Information)"
            ])

            if feature_selection_method == "Manual selection":
                selected_features = st.multiselect(
                    "Select features for the model",
                    potential_features,
                    default=potential_features[:min(5, len(potential_features)
                                                    )])
            else:  # Automatic selection
                # Get target variable
                y = df[target_col].values

                # Handle missing values...
                if pd.isna(y).any():
                    st.warning(
                        "Target variable has missing values. Rows with missing targets will be dropped."
                    )
                    valid_idx = ~pd.isna(y)
                    X_sel = df[potential_features].loc[valid_idx].copy()
                    y_sel = y[valid_idx]
                else:
                    X_sel = df[potential_features].copy()
                    y_sel = y

                if X_sel.isnull().any().any():
                    st.warning(
                        "Features contain missing values. These will be replaced with mean values for feature selection."
                    )
                    X_sel = X_sel.fillna(X_sel.mean())

                # Number of features to select
                k_features = st.slider(
                    "Number of features to select automatically",
                    min_value=2,
                    max_value=min(20, len(potential_features)),
                    value=min(5, len(potential_features)))

                # üî• CHOOSE selector based on method
                try:
                    if feature_selection_method == "Automatic selection (ANOVA F-value)":
                        selector = SelectKBest(score_func=f_classif,
                                               k=k_features)
                    elif feature_selection_method == "Automatic selection (Mutual Information)":
                        selector = SelectKBest(score_func=mutual_info_classif,
                                               k=k_features)

                    selector.fit(X_sel, y_sel)

                    # Results
                    feature_scores = pd.DataFrame({
                        'Feature': potential_features,
                        'Score': selector.scores_
                    })
                    feature_scores = feature_scores.sort_values(
                        'Score', ascending=False)

                    # Display feature scores
                    st.write("Feature Scores:")
                    st.dataframe(feature_scores.round(4),
                                 use_container_width=True)

                    # Select best features
                    selected_features = feature_scores.iloc[:k_features][
                        'Feature'].tolist()

                    st.write("Selected Features:")
                    st.write(", ".join(selected_features))

                    fig = px.bar(feature_scores.iloc[:k_features],
                                 x='Feature',
                                 y='Score',
                                 title="Feature Importance Scores")
                    st.plotly_chart(fig, use_container_width=True)

                except Exception as e:
                    st.error(f"Error in automatic feature selection: {str(e)}")
                    selected_features = st.multiselect(
                        "Select features for the model",
                        potential_features,
                        default=potential_features[:min(
                            5, len(potential_features))])

            if len(selected_features) < 2:
                st.warning(
                    "Please select at least 2 features to build a model.")
            else:
                # 3. Split dataset and preprocessing options
                st.subheader("3. Data Preprocessing and Splitting")

                # Create feature matrix and target vector
                X = df[selected_features].copy()
                y = df[target_col].copy()

                # Check for missing values
                if X.isnull().any().any() or y.isnull().any():
                    st.warning(
                        "Data contains missing values. Rows with any missing values will be removed."
                    )
                    # Drop rows with missing values
                    valid_idx = ~(X.isnull().any(axis=1) | y.isnull())
                    X = X.loc[valid_idx]
                    y = y.loc[valid_idx]
                    st.info(
                        f"Remaining data after removing missing values: {len(X)} rows"
                    )

                if len(X) == 0:
                    st.error(
                        "No data remaining after handling missing values. Please revise your feature selection or handle missing values differently."
                    )
                else:
                    # Preprocessing options
                    preprocessing = st.checkbox(
                        "Apply feature standardization (recommended)",
                        value=True)

                    # Model selection
                    st.subheader("4. Select Classification Model")
                    model_type = st.selectbox(
                        "Choose a classification model", [
                            "Logistic Regression", "Random Forest",
                            "Support Vector Machine (SVM)", "XGBoost",
                            "K-Nearest Neighbors (KNN)"
                        ])

                    # Configure model hyperparameters
                    st.subheader("5. Model Configuration")

                    if model_type == "Logistic Regression":
                        C = st.slider("Regularization strength (C)", 0.01,
                                      10.0, 1.0, 0.01)
                        model = LogisticRegression(C=C,
                                                   max_iter=1000,
                                                   random_state=42)

                    elif model_type == "Random Forest":
                        n_estimators = st.slider("Number of trees", 10, 500,
                                                 100, 10)
                        max_depth = st.slider("Maximum tree depth", 1, 30, 10)
                        model = RandomForestClassifier(
                            n_estimators=n_estimators,
                            max_depth=max_depth,
                            random_state=42)

                    elif model_type == "Support Vector Machine (SVM)":
                        C = st.slider("Regularization parameter (C)", 0.1,
                                      10.0, 1.0, 0.1)
                        kernel = st.selectbox("Kernel type",
                                              ["linear", "rbf", "poly"])
                        model = SVC(C=C,
                                    kernel=kernel,
                                    probability=True,
                                    random_state=42)

                    elif model_type == "XGBoost":
                        n_estimators = st.slider("Number of boosting rounds",
                                                 10, 500, 100, 10)
                        max_depth = st.slider("Maximum tree depth", 1, 15, 6)
                        learning_rate = st.slider("Learning rate", 0.01, 0.3,
                                                  0.1, 0.01)
                        model = XGBClassifier(n_estimators=n_estimators,
                                              max_depth=max_depth,
                                              learning_rate=learning_rate,
                                              random_state=42)

                    elif model_type == "K-Nearest Neighbors (KNN)":
                        n_neighbors = st.slider("Number of neighbors", 1, 50,
                                                5)
                        model = KNeighborsClassifier(n_neighbors=n_neighbors)

                    # Cross-validation options
                    st.subheader("6. Cross-Validation Settings")
                    cv_folds = st.slider("Number of cross-validation folds", 2,
                                         10, 5)

                    # Training and evaluation button
                    if st.button("Train and Evaluate Model"):
                        try:
                            # Status message
                            status = st.empty()
                            status.info("Training in progress... Please wait.")

                            # Preprocessing
                            # Convert categorical target to numeric
                            label_encoder = LabelEncoder()
                            y_encoded = label_encoder.fit_transform(y)

                            # Standardize features if selected
                            if preprocessing:
                                scaler = StandardScaler()
                                X_processed = scaler.fit_transform(X)
                            else:
                                X_processed = X.values

                            # Prepare cross-validation
                            cv = StratifiedKFold(n_splits=cv_folds,
                                                 shuffle=True,
                                                 random_state=42)

                            # Initialize metrics tracking
                            fold_accuracies = []
                            fold_aucs = []
                            all_y_true = []
                            all_y_pred = []
                            all_y_prob = []

                            # Get number of classes
                            n_classes = len(np.unique(y_encoded))
                            is_binary = n_classes == 2

                            # Perform cross-validation
                            for fold, (train_idx, test_idx) in enumerate(
                                    cv.split(X_processed, y_encoded)):
                                # Split data
                                X_train, X_test = X_processed[
                                    train_idx], X_processed[test_idx]
                                y_train, y_test = y_encoded[
                                    train_idx], y_encoded[test_idx]

                                # Train model
                                model.fit(X_train, y_train)

                                # Predict
                                y_pred = model.predict(X_test)

                                # Get probabilities
                                if hasattr(model, "predict_proba"):
                                    y_prob = model.predict_proba(X_test)
                                else:
                                    # Some models like SVM without probability might not have predict_proba
                                    y_prob = np.zeros((len(y_test), n_classes))
                                    y_prob[np.arange(len(y_test)), y_pred] = 1

                                # Calculate metrics
                                fold_acc = accuracy_score(y_test, y_pred)
                                fold_accuracies.append(fold_acc)

                                # Store predictions for later use
                                all_y_true.extend(y_test)
                                all_y_pred.extend(y_pred)

                                # For binary classification, compute AUC
                                if is_binary:
                                    if y_prob.shape[
                                            1] == 2:  # Binary with probabilities for both classes
                                        fold_auc = roc_auc_score(
                                            y_test, y_prob[:, 1])
                                        all_y_prob.extend(y_prob[:, 1])
                                    else:  # In case the probability is provided differently
                                        fold_auc = roc_auc_score(
                                            y_test, y_pred)
                                        all_y_prob.extend(y_pred)
                                    fold_aucs.append(fold_auc)

                            # Calculate average metrics
                            avg_accuracy = np.mean(fold_accuracies)

                            # Convert to numpy arrays for easier handling
                            all_y_true = np.array(all_y_true)
                            all_y_pred = np.array(all_y_pred)

                            # Update status
                            status.success(
                                "Training complete! Displaying results...")

                            # Display results
                            st.subheader("Model Performance")

                            metrics_col1, metrics_col2 = st.columns(2)
                            with metrics_col1:
                                st.metric("Average Accuracy",
                                          f"{avg_accuracy:.4f}")
                            with metrics_col2:
                                if is_binary:
                                    avg_auc = np.mean(fold_aucs)
                                    st.metric("Average AUC-ROC",
                                              f"{avg_auc:.4f}")

                            # Fold-wise accuracies
                            fold_df = pd.DataFrame({
                                'Fold':
                                [f"Fold {i+1}" for i in range(cv_folds)],
                                'Accuracy':
                                fold_accuracies
                            })
                            if is_binary:
                                fold_df['AUC-ROC'] = fold_aucs

                            st.write("Cross-Validation Results:")
                            st.dataframe(fold_df.round(4),
                                         use_container_width=True)

                            # Confusion Matrix
                            st.subheader("Confusion Matrix")
                            conf_matrix = confusion_matrix(
                                all_y_true, all_y_pred)

                            # Get class labels
                            class_labels = label_encoder.inverse_transform(
                                np.unique(all_y_true))

                            # Create confusion matrix heatmap
                            conf_fig = px.imshow(conf_matrix,
                                                 x=class_labels,
                                                 y=class_labels,
                                                 labels=dict(x="Predicted",
                                                             y="True",
                                                             color="Count"),
                                                 text_auto=True,
                                                 title="Confusion Matrix")
                            st.plotly_chart(conf_fig, use_container_width=True)

                            # For binary classification, show ROC curve
                            if is_binary and len(all_y_prob) > 0:
                                st.subheader("ROC Curve")
                                all_y_prob = np.array(all_y_prob)
                                fpr, tpr, _ = roc_curve(all_y_true, all_y_prob)
                                roc_auc = auc(fpr, tpr)

                                roc_fig = px.line(
                                    x=fpr,
                                    y=tpr,
                                    labels={
                                        "x": "False Positive Rate",
                                        "y": "True Positive Rate"
                                    },
                                    title=f"ROC Curve (AUC = {roc_auc:.4f})")
                                roc_fig.add_shape(type='line',
                                                  line=dict(dash='dash'),
                                                  x0=0,
                                                  x1=1,
                                                  y0=0,
                                                  y1=1)
                                st.plotly_chart(roc_fig,
                                                use_container_width=True)

                                # Precision-Recall curve
                                st.subheader("Precision-Recall Curve")
                                precision, recall, _ = precision_recall_curve(
                                    all_y_true, all_y_prob)
                                avg_precision = average_precision_score(
                                    all_y_true, all_y_prob)

                                pr_fig = px.line(
                                    x=recall,
                                    y=precision,
                                    labels={
                                        "x": "Recall",
                                        "y": "Precision"
                                    },
                                    title=
                                    f"Precision-Recall Curve (AP = {avg_precision:.4f})"
                                )
                                st.plotly_chart(pr_fig,
                                                use_container_width=True)

                            # Feature importance (for tree-based models)
                            if model_type in ["Random Forest", "XGBoost"]:
                                st.subheader("Feature Importance")

                                if model_type == "Random Forest":
                                    importances = model.feature_importances_
                                elif model_type == "XGBoost":
                                    importances = model.feature_importances_

                                feature_importance = pd.DataFrame({
                                    'Feature':
                                    selected_features,
                                    'Importance':
                                    importances
                                })
                                feature_importance = feature_importance.sort_values(
                                    'Importance', ascending=False)

                                imp_fig = px.bar(feature_importance,
                                                 x='Feature',
                                                 y='Importance',
                                                 title="Feature Importance")
                                st.plotly_chart(imp_fig,
                                                use_container_width=True)

                            # For logistic regression, show coefficients
                            elif model_type == "Logistic Regression":
                                st.subheader("Model Coefficients")

                                # Train model on full dataset to get coefficients
                                model.fit(X_processed, y_encoded)

                                coefficients = model.coef_

                                if len(coefficients.shape
                                       ) == 2 and coefficients.shape[0] == 1:
                                    # Binary classification
                                    coef_df = pd.DataFrame({
                                        'Feature':
                                        selected_features,
                                        'Coefficient':
                                        coefficients[0]
                                    })
                                else:
                                    # Multiclass - showing coefficients for each class
                                    coef_df = pd.DataFrame()
                                    for i, class_label in enumerate(
                                            label_encoder.classes_):
                                        if i < coefficients.shape[
                                                0]:  # Check if class has coefficients
                                            for j, feature in enumerate(
                                                    selected_features):
                                                coef_df = pd.concat([
                                                    coef_df,
                                                    pd.DataFrame({
                                                        'Class': [class_label],
                                                        'Feature': [feature],
                                                        'Coefficient':
                                                        [coefficients[i, j]]
                                                    })
                                                ])

                                coef_df = coef_df.sort_values('Coefficient',
                                                              ascending=False)
                                st.dataframe(coef_df.round(4),
                                             use_container_width=True)

                                # Visualize coefficients
                                if 'Class' in coef_df.columns:
                                    # Multiclass
                                    coef_fig = px.bar(
                                        coef_df,
                                        x='Feature',
                                        y='Coefficient',
                                        color='Class',
                                        title=
                                        "Logistic Regression Coefficients by Class"
                                    )
                                else:
                                    # Binary
                                    coef_fig = px.bar(
                                        coef_df,
                                        x='Feature',
                                        y='Coefficient',
                                        title="Logistic Regression Coefficients"
                                    )
                                st.plotly_chart(coef_fig,
                                                use_container_width=True)

                            # Save the trained model option
                            st.subheader("Save Trained Model")

                            # Train model on full dataset for saving
                            final_model = model
                            final_model.fit(X_processed, y_encoded)

                            # Serialize the model, preprocessing components, and metadata
                            model_data = {
                                'model': final_model,
                                'features': selected_features,
                                'target': target_col,
                                'target_classes': list(label_encoder.classes_),
                                'preprocessing': preprocessing,
                                'scaler': scaler if preprocessing else None,
                                'label_encoder': label_encoder,
                                'model_type': model_type
                            }

                            # Serialize to bytes for download
                            model_bytes = io.BytesIO()
                            joblib.dump(model_data, model_bytes)
                            model_bytes.seek(0)

                            st.download_button(
                                "Download Trained Model",
                                data=model_bytes,
                                file_name=
                                f"{model_type.lower().replace(' ', '_')}_model.joblib",
                                mime="application/octet-stream",
                            )

                        except Exception as e:
                            st.error(f"Error during model training: {str(e)}")
                            st.exception(e)

            # ========================== 7. Inference on New Data ==========================
            st.subheader("7. Inference on New Data")

            # Upload trained model
            uploaded_model_file = st.file_uploader(
                "Upload a saved model (.joblib)", type=["joblib"])

            # Upload CSV for inference
            uploaded_data_file = st.file_uploader(
                "Upload a CSV file for prediction", type=["csv"])

            if uploaded_model_file and uploaded_data_file:
                try:
                    # Load the saved model and metadata
                    loaded_model_data = joblib.load(uploaded_model_file)

                    # Extract components
                    model_loaded = loaded_model_data['model']
                    features_loaded = loaded_model_data['features']
                    target_loaded = loaded_model_data['target']
                    scaler_loaded = loaded_model_data['scaler']
                    label_encoder_loaded = loaded_model_data['label_encoder']
                    preprocessing_loaded = loaded_model_data['preprocessing']

                    # Read the uploaded CSV
                    inference_df = pd.read_csv(uploaded_data_file)

                    # Check if required features exist
                    missing_features = [
                        f for f in features_loaded
                        if f not in inference_df.columns
                    ]
                    if missing_features:
                        st.error(
                            f"The uploaded data is missing the following required features: {', '.join(missing_features)}"
                        )
                    else:
                        # Subset the dataframe to required features
                        X_infer = inference_df[features_loaded].copy()

                        # Handle missing values
                        if X_infer.isnull().any().any():
                            st.warning(
                                "Uploaded data contains missing values. They will be filled with feature means."
                            )
                            X_infer = X_infer.fillna(X_infer.mean())

                        # Apply preprocessing if needed
                        if preprocessing_loaded and scaler_loaded is not None:
                            X_infer = scaler_loaded.transform(X_infer)
                        else:
                            X_infer = X_infer.values

                        # Perform predictions
                        y_pred_infer = model_loaded.predict(X_infer)
                        y_pred_labels = label_encoder_loaded.inverse_transform(
                            y_pred_infer)

                        # Show prediction results
                        st.success("Inference complete!")
                        result_df = inference_df.copy()
                        result_df['Predicted Label'] = y_pred_labels

                        st.dataframe(result_df, use_container_width=True)

                        # Download predictions
                        csv_result = result_df.to_csv(
                            index=False).encode('utf-8')
                        st.download_button(label="üì• Download Predictions CSV",
                                           data=csv_result,
                                           file_name="predictions.csv",
                                           mime="text/csv")

                except Exception as e:
                    st.error(
                        f"Error loading model or performing inference: {str(e)}"
                    )
                    st.exception(e)
            else:
                st.info(
                    "Please upload both a trained model (.joblib) and a CSV file for inference."
                )

# Image Analysis block
elif selected_page == "Image Analysis":
    st.title("üñºÔ∏è Image Analysis")

    # Create tabs for different image types
    tab1, tab2 = st.tabs(
        ["Medical Images (NIfTI)", "Natural Images (JPG/PNG)"])

    with tab1:
        st.header("Medical Image Analysis")

        st.info(
            "Upload NIfTI (.nii or .nii.gz) files to analyze and segment them with pre-trained models."
        )

        uploaded_file = st.file_uploader("Upload a NIfTI file",
                                         type=["nii", "nii.gz"],
                                         key="nifti_uploader")

        if uploaded_file:
            # Save the uploaded file temporarily
            with tempfile.NamedTemporaryFile(delete=False,
                                             suffix=".nii.gz") as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                temp_nifti_path = tmp_file.name

            try:
                # Load the image
                st.write("Loading image...")
                img_sitk = sitk.ReadImage(temp_nifti_path)
                img_np = sitk.GetArrayFromImage(img_sitk)

                st.success(f"Image loaded successfully! Shape: {img_np.shape}")

                # Image properties
                spacing = img_sitk.GetSpacing()
                origin = img_sitk.GetOrigin()
                direction = img_sitk.GetDirection()

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Width", f"{img_np.shape[2]} px")
                with col2:
                    st.metric("Height", f"{img_np.shape[1]} px")
                with col3:
                    st.metric("Slices", f"{img_np.shape[0]}")

                # Add some spacing
                st.write("")

                # Image details in an expander
                with st.expander("Image Details"):
                    st.write(f"**Spacing:** {spacing}")
                    st.write(f"**Origin:** {origin}")
                    st.write(f"**Direction:** {direction}")
                    st.write(
                        f"**Value Range:** [{np.min(img_np)}, {np.max(img_np)}]"
                    )

                # Image viewer with slider
                st.subheader("Image Viewer")

                max_slice = img_np.shape[0] - 1
                slice_idx = st.slider("Slice", 0, max_slice, max_slice // 2)

                window_center = st.slider("Window Center", -1000, 3000, 40)
                window_width = st.slider("Window Width", 1, 4000, 400)

                # Apply windowing
                img_slice = img_np[slice_idx].copy().astype(float)
                img_slice = np.clip(img_slice,
                                    window_center - window_width // 2,
                                    window_center + window_width // 2)
                img_slice = (
                    img_slice -
                    (window_center - window_width // 2)) / window_width

                # Display the slice
                fig, ax = plt.subplots(figsize=(10, 10))
                ax.imshow(img_slice, cmap='gray')
                ax.axis('off')
                st.pyplot(fig)

                # Segmentation options
                st.subheader("Image Segmentation")

                segmentation_method = st.radio(
                    "Select segmentation method",
                    ["LungMask (Lungs)", "TotalSegmentator (Various Organs)"])

                if segmentation_method == "LungMask (Lungs)":
                    if st.button("Segment Lungs"):
                        try:
                            st.write(
                                "Running lung segmentation... This may take a moment."
                            )

                            # Run segmentation
                            seg_np = mask.apply(img_sitk)

                            # Create overlay
                            overlay = overlay_mask_on_image(
                                img_np[slice_idx], seg_np[slice_idx])

                            # Display result
                            fig, ax = plt.subplots(figsize=(10, 10))
                            ax.imshow(overlay)
                            ax.axis('off')
                            st.pyplot(fig)

                            # Save result option
                            with tempfile.NamedTemporaryFile(
                                    delete=False, suffix=".png") as tmp_out:
                                plt.imsave(tmp_out.name, overlay)

                                with open(tmp_out.name, "rb") as f:
                                    st.download_button(
                                        "Download Segmentation Result",
                                        f,
                                        file_name="lung_segmentation.png",
                                        mime="image/png")

                        except Exception as e:
                            st.error(f"Segmentation error: {str(e)}")

                elif segmentation_method == "TotalSegmentator (Various Organs)":
                    # Organ selection
                    organ_options = [
                        "liver", "spleen", "kidney_right", "kidney_left",
                        "stomach", "pancreas", "lung_right", "lung_left",
                        "heart", "aorta", "brain", "bladder"
                    ]

                    selected_organ = st.selectbox("Select organ to segment",
                                                  organ_options)
                    mask_color = st.color_picker("Select mask color",
                                                 "#FF0000")

                    if st.button("Segment Selected Organ"):
                        try:
                            st.write(
                                f"Running segmentation for **{selected_organ}**... Please wait."
                            )

                            with tempfile.TemporaryDirectory() as tmpdir:
                                input_path = os.path.join(
                                    tmpdir, "input.nii.gz")
                                output_dir = os.path.join(tmpdir, "output")

                                # Save input image
                                sitk.WriteImage(img_sitk, input_path)

                                # Run TotalSegmentator
                                totalsegmentator(input=input_path,
                                                 output=output_dir,
                                                 task="total",
                                                 roi_subset=[selected_organ],
                                                 fast=True)

                                # Check if output exists
                                organ_mask_path = os.path.join(
                                    output_dir, f"{selected_organ}.nii.gz")
                                if os.path.exists(organ_mask_path):
                                    organ_mask = sitk.ReadImage(
                                        organ_mask_path)
                                    mask_np = sitk.GetArrayFromImage(
                                        organ_mask)

                                    # Create overlay
                                    overlay = overlay_mask_on_image(
                                        img_np[slice_idx],
                                        mask_np[slice_idx],
                                        color=mask_color)

                                    # Display original and overlayed images
                                    st.markdown("### üß† Segmentation Result")
                                    col1, col2 = st.columns(2)
                                    with col1:
                                        st.markdown("**Original Image**")
                                        fig1, ax1 = plt.subplots(figsize=(4,
                                                                          4))
                                        ax1.imshow(img_np[slice_idx],
                                                   cmap="gray")
                                        ax1.axis("off")
                                        st.pyplot(fig1)
                                    with col2:
                                        st.markdown("**Overlayed Mask**")
                                        fig2, ax2 = plt.subplots(figsize=(4,
                                                                          4))
                                        ax2.imshow(overlay)
                                        ax2.axis("off")
                                        st.pyplot(fig2)

                                    # Download segmented mask
                                    segmented_mask_path = os.path.join(
                                        tmpdir, f"seg_{selected_organ}.nii.gz")
                                    sitk.WriteImage(organ_mask,
                                                    segmented_mask_path)

                                    with open(segmented_mask_path, "rb") as f:
                                        st.download_button(
                                            label=
                                            f"üì• Download {selected_organ} Segmentation (.nii.gz)",
                                            data=f,
                                            file_name=
                                            f"seg_{selected_organ}.nii.gz",
                                            mime="application/gzip")
                                else:
                                    st.error(
                                        f"Segmentation failed. Organ mask not found for: {selected_organ}"
                                    )

                        except Exception as e:
                            st.error(f"Segmentation error: {str(e)}")

            except Exception as e:
                st.error(f"Error processing image: {str(e)}")
            finally:
                # Clean up temporary file
                try:
                    os.unlink(temp_nifti_path)
                except:
                    pass

    with tab2:
        st.header("Natural Image Analysis")
        st.info(
            "Upload standard image files like JPG or PNG to analyze and process them."
        )

        uploaded_file = st.file_uploader("Upload an image",
                                         type=["jpg", "jpeg", "png"],
                                         key="natural_img_uploader")

        if uploaded_file:
            # Read the image
            try:
                file_bytes = np.asarray(bytearray(uploaded_file.read()),
                                        dtype=np.uint8)
                img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
                img = cv2.cvtColor(img,
                                   cv2.COLOR_BGR2RGB)  # Convert BGR to RGB

                st.success(f"Image loaded successfully! Shape: {img.shape}")

                # Display original image
                st.subheader("Original Image")
                st.image(img, use_column_width=True)

                # Image processing options
                st.subheader("Image Processing")

                processing_options = st.multiselect(
                    "Select processing operations", [
                        "Grayscale Conversion", "Channel Separation",
                        "Blur/Smooth", "Face Detection", "Edge Detection",
                        "Thresholding", "Histogram Equalization"
                    ])

                processed_img = img.copy()

                # Process based on options
                if "Grayscale Conversion" in processing_options:
                    gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
                    processed_img = gray_img

                    st.subheader("Grayscale Image")
                    st.image(gray_img, use_column_width=True)

                if "Channel Separation" in processing_options:
                    st.subheader("Color Channels")
                    r, g, b = cv2.split(img)

                    # Create colored versions
                    zeros = np.zeros_like(r)

                    red_img = cv2.merge([r, zeros, zeros])  # Red only
                    green_img = cv2.merge([zeros, g, zeros])  # Green only
                    blue_img = cv2.merge([zeros, zeros, b])  # Blue only

                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.write("Red Channel")
                        st.image(cv2.cvtColor(red_img, cv2.COLOR_BGR2RGB),
                                 use_column_width=True)
                    with col2:
                        st.write("Green Channel")
                        st.image(cv2.cvtColor(green_img, cv2.COLOR_BGR2RGB),
                                 use_column_width=True)
                    with col3:
                        st.write("Blue Channel")
                        st.image(cv2.cvtColor(blue_img, cv2.COLOR_BGR2RGB),
                                 use_column_width=True)

                if "Blur/Smooth" in processing_options:
                    st.subheader("Image Smoothing")

                    blur_method = st.radio(
                        "Select blur method",
                        ["Gaussian Blur", "Median Blur", "Bilateral Filter"])

                    if blur_method == "Gaussian Blur":
                        kernel_size = st.slider("Kernel Size",
                                                1,
                                                31,
                                                5,
                                                step=2)
                        blurred = cv2.GaussianBlur(img,
                                                   (kernel_size, kernel_size),
                                                   0)
                        st.image(blurred,
                                 caption="Gaussian Blur",
                                 use_column_width=True)
                        processed_img = blurred

                    elif blur_method == "Median Blur":
                        kernel_size = st.slider("Kernel Size",
                                                1,
                                                31,
                                                5,
                                                step=2)
                        blurred = cv2.medianBlur(img, kernel_size)
                        st.image(blurred,
                                 caption="Median Blur",
                                 use_column_width=True)
                        processed_img = blurred

                    elif blur_method == "Bilateral Filter":
                        d = st.slider("Diameter", 1, 30, 9)
                        sigma_color = st.slider("Sigma Color", 1, 200, 75)
                        sigma_space = st.slider("Sigma Space", 1, 200, 75)
                        blurred = cv2.bilateralFilter(img, d, sigma_color,
                                                      sigma_space)
                        st.image(blurred,
                                 caption="Bilateral Filter",
                                 use_column_width=True)
                        processed_img = blurred

                if "Face Detection" in processing_options:
                    st.subheader("Face Detection")
                    face_cascade = cv2.CascadeClassifier(
                        cv2.data.haarcascades +
                        "haarcascade_frontalface_default.xml")
                    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                    faces = face_cascade.detectMultiScale(gray, 1.1, 4)
                    face_img = img.copy()
                    for (x, y, w, h) in faces:
                        cv2.rectangle(face_img, (x, y), (x + w, y + h),
                                      (255, 0, 0), 2)
                    st.image(face_img,
                             caption="Face Detection",
                             use_column_width=True)

                if "Edge Detection" in processing_options:
                    st.subheader("Edge Detection")

                    edge_method = st.radio("Select edge detection method",
                                           ["Canny", "Sobel", "Laplacian"])

                    # Convert to grayscale if not already
                    if len(processed_img.shape) == 3:
                        gray = cv2.cvtColor(processed_img, cv2.COLOR_RGB2GRAY)
                    else:
                        gray = processed_img

                    if edge_method == "Canny":
                        low_threshold = st.slider("Low Threshold", 0, 255, 50)
                        high_threshold = st.slider("High Threshold", 0, 255,
                                                   150)
                        edges = cv2.Canny(gray, low_threshold, high_threshold)
                        st.image(edges,
                                 caption="Canny Edge Detection",
                                 use_column_width=True)
                        processed_img = edges

                    elif edge_method == "Sobel":
                        ksize = st.slider("Kernel Size", 1, 7, 3, step=2)
                        sobel_x = cv2.Sobel(gray,
                                            cv2.CV_64F,
                                            1,
                                            0,
                                            ksize=ksize)
                        sobel_y = cv2.Sobel(gray,
                                            cv2.CV_64F,
                                            0,
                                            1,
                                            ksize=ksize)
                        sobel_x = cv2.convertScaleAbs(sobel_x)
                        sobel_y = cv2.convertScaleAbs(sobel_y)
                        sobel_combined = cv2.addWeighted(
                            sobel_x, 0.5, sobel_y, 0.5, 0)
                        st.image(sobel_combined,
                                 caption="Sobel Edge Detection",
                                 use_column_width=True)
                        processed_img = sobel_combined

                    elif edge_method == "Laplacian":
                        ksize = st.slider("Kernel Size", 1, 7, 3, step=2)
                        laplacian = cv2.Laplacian(gray,
                                                  cv2.CV_64F,
                                                  ksize=ksize)
                        laplacian = cv2.convertScaleAbs(laplacian)
                        st.image(laplacian,
                                 caption="Laplacian Edge Detection",
                                 use_column_width=True)
                        processed_img = laplacian

                if "Thresholding" in processing_options:
                    st.subheader("Thresholding")

                    # Convert to grayscale if not already
                    if len(processed_img.shape) == 3:
                        gray = cv2.cvtColor(processed_img, cv2.COLOR_RGB2GRAY)
                    else:
                        gray = processed_img

                    thresh_method = st.radio("Select thresholding method", [
                        "Binary", "Binary Inverted", "Truncated", "To Zero",
                        "To Zero Inverted", "Adaptive Mean",
                        "Adaptive Gaussian", "Otsu's"
                    ])

                    if thresh_method in [
                            "Binary", "Binary Inverted", "Truncated",
                            "To Zero", "To Zero Inverted"
                    ]:
                        threshold_value = st.slider("Threshold Value", 0, 255,
                                                    127)

                        methods = {
                            "Binary": cv2.THRESH_BINARY,
                            "Binary Inverted": cv2.THRESH_BINARY_INV,
                            "Truncated": cv2.THRESH_TRUNC,
                            "To Zero": cv2.THRESH_TOZERO,
                            "To Zero Inverted": cv2.THRESH_TOZERO_INV
                        }

                        _, thresh = cv2.threshold(gray, threshold_value, 255,
                                                  methods[thresh_method])
                        st.image(thresh,
                                 caption=f"{thresh_method} Thresholding",
                                 use_column_width=True)
                        processed_img = thresh

                    elif thresh_method in [
                            "Adaptive Mean", "Adaptive Gaussian"
                    ]:
                        block_size = st.slider("Block Size", 3, 51, 11, step=2)
                        C = st.slider("C (Constant subtracted from mean)", -50,
                                      50, 2)

                        if thresh_method == "Adaptive Mean":
                            thresh = cv2.adaptiveThreshold(
                                gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,
                                cv2.THRESH_BINARY, block_size, C)
                        else:
                            thresh = cv2.adaptiveThreshold(
                                gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                cv2.THRESH_BINARY, block_size, C)

                        st.image(thresh,
                                 caption=f"{thresh_method} Thresholding",
                                 use_column_width=True)
                        processed_img = thresh

                    elif thresh_method == "Otsu's":
                        _, thresh = cv2.threshold(
                            gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
                        st.image(thresh,
                                 caption="Otsu's Thresholding",
                                 use_column_width=True)
                        processed_img = thresh

                if "Histogram Equalization" in processing_options:
                    st.subheader("Histogram Equalization")

                    # Convert to grayscale if not already
                    if len(processed_img.shape) == 3:
                        if "Grayscale Conversion" in processing_options:
                            # Use the already converted grayscale image
                            equalized = cv2.equalizeHist(processed_img)
                        else:
                            # Convert RGB to YUV and equalize only the Y channel
                            img_yuv = cv2.cvtColor(processed_img,
                                                   cv2.COLOR_RGB2YUV)
                            img_yuv[:, :, 0] = cv2.equalizeHist(img_yuv[:, :,
                                                                        0])
                            equalized = cv2.cvtColor(img_yuv,
                                                     cv2.COLOR_YUV2RGB)
                    else:
                        equalized = cv2.equalizeHist(processed_img)

                    st.image(equalized,
                             caption="Histogram Equalization",
                             use_column_width=True)
                    processed_img = equalized

                # Download processed image
                if len(processing_options) > 0:
                    # Encode processed image based on its type
                    if len(processed_img.shape) == 3:  # RGB image
                        is_success, buffer = cv2.imencode(
                            ".png",
                            cv2.cvtColor(processed_img, cv2.COLOR_RGB2BGR))
                    else:  # Grayscale image
                        is_success, buffer = cv2.imencode(
                            ".png", processed_img)

                    if is_success:
                        bytes_data = buffer.tobytes()
                        st.download_button(label="Download Processed Image",
                                           data=bytes_data,
                                           file_name="processed_image.png",
                                           mime="image/png")

            except Exception as e:
                st.error(f"Error processing image: {str(e)}")

elif selected_page == "AI Assistant":
    # Call the AI assistant module to display the chat interface
    display_ai_assistant()

else:
    st.title("üìä Interactive Biomedical Data Dashboard")
    st.warning("Please upload a dataset using the sidebar to begin.")
    st.markdown("""
    ### Supported file formats:
    - CSV (.csv)
    - TSV (.tsv)
    - Excel (.xls, .xlsx)
    """)
